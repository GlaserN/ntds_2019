{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'19] assignment 1: network science\n",
    "[ntds'19]: https://github.com/mdeff/ntds_2019\n",
    "\n",
    "[Clément Vignac](https://people.epfl.ch/clement.vignac), [EPFL LTS4](https://lts4.epfl.ch) and\n",
    "[Guillermo Ortiz Jiménez](https://gortizji.github.io), [EPFL LTS4](https://lts4.epfl.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `<your team number>`\n",
    "* Students: `<your name`> (for the indivudual submission) or `<the name of all students in the team>` (for the team submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "Grading:\n",
    "* The first deadline is for individual submissions. The second deadline is for the team submission.\n",
    "* All team members will receive the same grade based on the team solution submitted on the second deadline.\n",
    "* As a fallback, a team can ask for individual grading. In that case, solutions submitted on the first deadline are graded.\n",
    "* Collaboration between team members is encouraged. No collaboration between teams is allowed.\n",
    "\n",
    "Submission:\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "  Note that Networkx is imported in the second section and cannot be used in the first.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart Kernel and Run All Cells\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "*To be completed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Signal Processing\n",
    "\n",
    "In this part of the assignment we are going to familiarize ourselves with the main concepts in Graph Signal Processing. \n",
    "\n",
    "You can only use the following libraries for this part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygsp import graphs\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the graph that will serve as support for our signals. In this exercise we will use a nearest-neighbor graph constructed from the Stanford Bunny point cloud included in the PyGSP library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphs.Bunny()\n",
    "adjacency = np.asarray(G.W.todense())\n",
    "n_nodes = adjacency.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bunny(signal=None,title=''):\n",
    "    fig = plt.gcf()\n",
    "    ax = plt.gca()\n",
    "    if not isinstance(ax, Axes3D):\n",
    "        ax = plt.subplot(111, projection='3d')\n",
    "    if signal is not None:\n",
    "        signal = np.squeeze(signal)\n",
    "    p = ax.scatter(G.coords[:,0], G.coords[:,1], G.coords[:,2], c=signal, marker='o', s=5, cmap='RdBu_r')\n",
    "    ax.view_init(elev=-90,azim=90)\n",
    "    ax.dist = 7\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(title)\n",
    "    if signal is not None:\n",
    "        fig.colorbar(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111,projection='3d')\n",
    "plot_bunny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let us start by constructing the combinatorial and normalized graph laplacians from an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinatorial_laplacian(adjacency):\n",
    "    D = np.diag(np.sum(adjacency, 1)) # Degree matrix\n",
    "    return D - adjacency\n",
    "\n",
    "def normalized_laplacian(adjacency):\n",
    "    D_norm = np.diag(np.sum(adjacency, 1)**(-1/2)) \n",
    "    laplacian_combinatorial =  combinatorial_laplacian(adjacency)\n",
    "    return D_norm @ laplacian_combinatorial @ D_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_comb = combinatorial_laplacian(adjacency)\n",
    "laplacian_norm = normalized_laplacian(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Find the spectral decomposition of the two laplacians. Make sure that the eigenvalues and eigenvectors are in increasing order $\\lambda_0\\leq \\lambda_1\\leq \\dots \\leq \\lambda_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decomposition(laplacian):\n",
    "    return np.linalg.eigh(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_comb, U_comb = spectral_decomposition(laplacian_comb)\n",
    "e_norm, U_norm = spectral_decomposition(laplacian_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121,)\n",
    "plt.plot(e_comb)\n",
    "plt.title('Eigenvalues $L_{comb}$')\n",
    "plt.subplot(122)\n",
    "plt.plot(e_norm)\n",
    "plt.title('Eigenvalues $L_{norm}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Can you say what is the relationship between the two plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus now on the normalized laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = laplacian_norm\n",
    "U = U_norm\n",
    "e = e_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more clear we will plot some of its eigenvectors (0, 1, 3, 10, 100) as signals on the bunny graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plt.subplot(231, projection='3d')\n",
    "plot_bunny(signal=U[:,0], title='Eigenvector #0')\n",
    "plt.subplot(232, projection='3d')\n",
    "plot_bunny(signal=U[:,1], title='Eigenvector #1')\n",
    "plt.subplot(233, projection='3d')\n",
    "plot_bunny(signal=U[:,2], title='Eigenvector #2')\n",
    "\n",
    "plt.subplot(234, projection='3d')\n",
    "plot_bunny(signal=U[:,3], title='Eigenvector #3')\n",
    "plt.subplot(235, projection='3d')\n",
    "plot_bunny(signal=U[:,10], title='Eigenvector #10')\n",
    "plt.subplot(236, projection='3d')\n",
    "plot_bunny(signal=U[:,100], title='Eigenvector #100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say in terms of the variations/smoothness of these signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat this experiment with the combinatorial laplacian, can we expect the same behaviour? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create a function to compute the Graph Fourier Transform and its inverse of a graph signal. **Note**: You can assume that you have internal access to the eigendecomposition (`U` and `e`) of the laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GFT(x):\n",
    "    return U.T @ x\n",
    "\n",
    "def iGFT(x):\n",
    "    return U @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an all-pass graph signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = iGFT(np.ones([n_nodes, 1]))\n",
    "plot_bunny(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot its graph spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(e, GFT(signal), use_line_collection=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter this signal using spectral templates. Let us start by creating three templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_lp = np.ones(signal.shape)\n",
    "template_bp = np.ones(signal.shape)\n",
    "template_hp = np.ones(signal.shape)\n",
    "\n",
    "template_lp[e >= 0.1] = 0 # Low-pass filter with cut-off at lambda=0.1\n",
    "template_bp[e < 0.1] = 0 # Band-pass filter with cut-offs at lambda=0.1 and lambda=0.5\n",
    "template_bp[e > 0.5] = 0\n",
    "template_hp[e <= 1] = 0 # High-pass filter with cut-off at lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to filter a signal given a specific spectral template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_graph_filter(signal, template):\n",
    "    signal_gft = GFT(signal)\n",
    "    filter_gft = signal_gft * template\n",
    "    return iGFT(filter_gft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_lp = template_graph_filter(signal,template_lp)\n",
    "signal_bp = template_graph_filter(signal,template_bp)\n",
    "signal_hp = template_graph_filter(signal,template_hp)\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "plt.subplot(131, projection='3d')\n",
    "plot_bunny(signal=signal_lp, title='Low-pass signal')\n",
    "plt.subplot(132, projection='3d')\n",
    "plot_bunny(signal=signal_bp, title='Band-pass signal')\n",
    "plt.subplot(133, projection='3d')\n",
    "plot_bunny(signal=signal_hp, title='High-pass signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say in terms of the signal variations? How would you link to the observations you made before about the spectral decomposition of the laplacian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "We have seen how we can use the GFT to define different filters that enhance or reduce certain frequency bands. However, to do so, we require an explicit eigendecomposition of the graph laplacian. For very large graphs this is very intense computationally. We will now see how we can obtain similar results by filtering the signals directly in the vertex domain.\n",
    "\n",
    "The key idea is to use a polynomial of the graph laplacian to define a graph filter, i.e., $g(L)x=\\sum_{k=1}^K \\alpha_k L^k x$, and use the fact that the powers of a diagonalizable matrix can be written in terms of powers of its eigenvalues. This is\n",
    "$$\n",
    "\\begin{equation}\n",
    "        L^k=(U\\Lambda U^T)^k=U\\Lambda^k U^T = U\\begin{bmatrix}\n",
    "        \\lambda_0^k &\\dots & 0\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        0 & \\dots & \\lambda_N\n",
    "        \\end{bmatrix} U^T\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "This means, that a polynomial of the graph laplacian acts independently on each eigenvalue of the graph, and has a frequency spectrum of\n",
    "$$g(\\lambda)=\\sum_{k=1}^K \\alpha_k \\lambda^k$$\n",
    "Hence,\n",
    "$$g(L)x=\\sum_{k=1}^K \\alpha_k L^k x=\\sum_{k=1}^K \\alpha_k U\\Lambda^k U^T x=U \\left(\\sum_{k=1}^K \\alpha_k\\Lambda^k \\right)U^T x=\\operatorname{iGFT}\\left(g(\\Lambda)\\operatorname{GFT}(x)\\right)$$\n",
    "\n",
    "\n",
    "With these ingredients, we have reduced the design of graph filters in the vertex domain to a regression task that approximates a given spectral response by a polynomial. There are multiple ways to do this, but in this assignment we will implement a very simple strategy based on [least-squares regression](https://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a band-pass universal spectral template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_grid = np.linspace(0,2, 200)\n",
    "\n",
    "template_bp = np.ones(freq_grid.shape)\n",
    "template_bp[freq_grid < 0.1] = 0\n",
    "template_bp[freq_grid >= 0.5] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to find the coefficients of a polynomial that approximates a given spectral template. **Hint:** `np.vander` and `np.linalg.lstsq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(freq_grid, order, template):\n",
    "    A = np.vander(freq_grid, order, increasing=True)\n",
    "    coeff = np.linalg.lstsq(A, template, rcond=None)[0]\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to compute the frequency response of that filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fir_graph_filter_response(coeff, freq_grid):\n",
    "    g = np.zeros_like(freq_grid)\n",
    "    for n, c in enumerate(coeff):\n",
    "        g += c * (freq_grid**n)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the band-pass template with several polynomials of different order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq_grid, template_bp)\n",
    "for order in [5, 10, 20, 30]:    \n",
    "    coeff_bp = fit_polynomial(freq_grid, order, template_bp)\n",
    "    plt.plot(freq_grid, fir_graph_filter_response(coeff_bp, freq_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous plot, choose a filter order that achieves (in your opinion) a good tradeoff in terms of computational complexity and response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_bp = fit_polynomial(freq_grid, order, template_bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only defined a way to compute the coefficients of our laplacian polynomial. Let us now compute our graph filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fir_graph_filter(coeff, laplacian):\n",
    "    g = 0\n",
    "    for n, c in enumerate(coeff):\n",
    "        g += c * np.linalg.matrix_power(laplacian, n)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_bp = fir_graph_filter(coeff_bp, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the previous all-pass signal with this filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_bp_fir = g_bp @ signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare with the previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121, projection='3d')\n",
    "plot_bunny(signal_bp, title='Spectral template')\n",
    "plt.subplot(122, projection='3d')\n",
    "plot_bunny(signal_bp_fir, title='FIR filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better compare these signals, let us plot their spectrums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(e, GFT(signal_bp), use_line_collection=True, linefmt='C0', markerfmt='C0')\n",
    "plt.stem(e, GFT(signal_bp_fir), use_line_collection=True, linefmt='C1', markerfmt='C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning on Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only played with toy examples. Let us see the use of these tools in practice! In particular, let us see how we can use some graph filters to construct features to feed a classifier. For this part of the assignment we will import some extra packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.data.citation_graph import load_cora\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CORA dataset and the citation graph that we created in Assignment 1. However, to simplify the next tasks we will directly use the preprocessed version of this dataset contained within the Deep Graph Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = load_cora()\n",
    "\n",
    "features = torch.FloatTensor(cora.features)\n",
    "labels = torch.LongTensor(cora.labels)\n",
    "\n",
    "train_mask = torch.BoolTensor(cora.train_mask)\n",
    "val_mask = torch.BoolTensor(cora.val_mask)\n",
    "test_mask = torch.BoolTensor(cora.test_mask)\n",
    "\n",
    "in_feats = features.shape[1]\n",
    "n_classes = cora.num_labels\n",
    "n_edges = cora.graph.number_of_edges()\n",
    "\n",
    "graph = cora.graph\n",
    "adjacency = np.asarray(nx.to_numpy_matrix(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use the combinatorial laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = combinatorial_laplacian(adjacency)\n",
    "U, e = spectral_decomposition(laplacian)\n",
    "e_max = np.max(e)\n",
    "freq_grid = np.linspace(0, e_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will interpret CORA's features as multidimensional graph signals living on the citation graph. Our task is to design a classifier that using these features and the geometry of the graph can identify the type of paper each node represents.\n",
    "\n",
    "We will start using a classical machine learning approach, and design some hand-crafted features to feed a logistic regression model. We will use GSP to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what you learned in class, your past experience, and your intuition, design a graph spectral template that you believe could enhance important features of the graph. \n",
    "\n",
    "**Note 1:** You just need to design one graph filter that we will apply to all features individually. \n",
    "\n",
    "**Note 2:** Finding the right filter can be very challenging, don't worry if you can't find it. Just make sure you experiment with a few configurations and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_template = np.ones(freq_grid.shape)\n",
    "\n",
    "filter_template[freq_grid >= 0.3] = 0 # Low-pass filter with cut-off at lambda=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a filter order to approximate your filter using laplacian polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 5\n",
    "\n",
    "coeff = fit_polynomial(freq_grid, order, filter_template)\n",
    "graph_filter = fir_graph_filter(coeff, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the frequency response of your spectral template and the polynomial approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq_grid, filter_template)\n",
    "plt.plot(freq_grid, fir_graph_filter_response(coeff, freq_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features = graph_filter @ features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our classifier we will select a few nodes in our graph for training and fit a [logistic regression classifier](https://en.wikipedia.org/wiki/Logistic_regression) on them. To avoid overfitting to the test set when we do hyperparameter tuning, we will also select a validation set. And finally, we will test our classifier on the rest of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = filtered_features[train_mask,:]\n",
    "train_labels = labels[train_mask]\n",
    "\n",
    "val_features = filtered_features[val_mask,:]\n",
    "val_labels = labels[val_mask]\n",
    "\n",
    "test_features = filtered_features[test_mask,:]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression classifier. Remember to play with the regularization parameters to achieve a well performing model. **Hint:** Use `sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', multi_class=\"auto\", solver=\"liblinear\", C=1e4, fit_intercept=False, max_iter=1000)\n",
    "log_reg.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate your model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = log_reg.score(train_features, train_labels)\n",
    "val_acc = log_reg.score(val_features, val_labels)\n",
    "test_acc = log_reg.score(test_features, test_labels)\n",
    "\n",
    "print('Train accuracy {:.4f} | Validation accuracy {:.4f} | Test accuracy {:.4f}'.format(train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you will probably have seen that it is very difficult to find the right combination of spectral response, filter parameters and regularization method. And in most cases, this is a painstaking job. Wouldn't it be great to automate these tasks?\n",
    "\n",
    "Fortunately, this is possible if we use the right tools! Specifically, we will see that Graph Convolutional Networks are a great framework to automatize the feature extraction method, by simply using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will follow the same classification pipeline as in Exercise 5, but now, instead of hand-crafting our filter we will let `PyTorch` find the coefficients using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by constructing a `LaplacianPolynomial` model in `DGL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacianPolynomial(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 k):\n",
    "        super().__init__()\n",
    "        self._in_feats = in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._k = k\n",
    "        self.pol_weights = nn.Parameter(torch.Tensor(self._k + 1))\n",
    "        self.logr_weights = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        dropout = 0.8\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        torch.nn.init.xavier_uniform_(self.logr_weights, gain=0.01)\n",
    "        torch.nn.init.normal_(self.pol_weights, mean=0.0, std=1e-3)\n",
    "\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        r\"\"\"Compute graph convolution.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n",
    "          dimensions, :math:`N` is the number of nodes.\n",
    "        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n",
    "          the same shape as the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph (DGLGraph) : The graph.\n",
    "        feat (torch.Tensor): The input feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor) The output feature\n",
    "        \"\"\"\n",
    "        feat = self.dropout(feat)\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        # mult W first to reduce the feature size for aggregation.\n",
    "        feat = torch.matmul(feat, self.logr_weights)\n",
    "\n",
    "        result = self.pol_weights[0] * feat.clone()\n",
    "\n",
    "        for i in range(1, self._k):\n",
    "            graph.ndata['h'] = feat\n",
    "            # Feat is not modified in place\n",
    "            graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "                             fn.sum(msg='m', out='h'))\n",
    "            feat = graph.in_degrees().float()[:, None] * feat - graph.ndata['h']\n",
    "            result += self.pol_weights[i] * feat\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"Set the extra representation of the module,\n",
    "        which will come into effect when printing the model.\n",
    "        \"\"\"\n",
    "        summary = 'in={_in_feats}, out={_out_feats}'\n",
    "        summary += ', normalization={_norm}'\n",
    "        return summary.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have are model ready we just need to create a function that performs one step of our training loop, and another one that evaluates our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, loss_fcn, train_mask, optimizer):\n",
    "    model.train()\n",
    "    # forward\n",
    "    logits = model(g, features)  # only compute the train set\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)[mask] # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the training parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_order = 3\n",
    "\n",
    "lr = 0.2\n",
    "weight_decay = 5e-6\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's train the classifier end to end. You should be able to obtain a test accuracy of more than 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = DGLGraph(cora.graph)\n",
    "\n",
    "model = LaplacianPolynomial(in_feats, n_classes, pol_order)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=lr,\n",
    "                             weight_decay=weight_decay)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    loss = train(model, graph, features, labels, loss_fcn, train_mask, optimizer)\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, graph, features, labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Accuracy {:.4f}\". format(\n",
    "            epoch, np.mean(dur), loss.item(), acc))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, graph, features, labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained this way our GCN based on polynomials of the laplacian is a black box. Fortunately, however, the only difference between this shallow model and our previous classifier is the way we chose the filter coefficients.\n",
    "\n",
    "Let's see what the network learnt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_gcn = model.pol_weights.detach().numpy()\n",
    "print(coeff_gcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the model we can plot the frequency response of the learned filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq_grid, fir_graph_filter_response(coeff_gcn, freq_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your previous results, what type of graph filter would you say this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct we should be able to use this filter to construct new hand-crafted filters and train a logistic regression model that achieves good accuracy on the training set. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the learned coefficients to train a new feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_gcn = fir_graph_filter(coeff_gcn, laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the new features by filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gcn = graph_gcn @ features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression on this features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_gcn = features_gcn[train_mask,:]\n",
    "train_labels = labels[train_mask]\n",
    "val_features_gcn = features_gcn[val_mask,:]\n",
    "val_labels = labels[val_mask]\n",
    "test_features_gcn = features_gcn[test_mask,:]\n",
    "test_labels = labels[test_mask]\n",
    "\n",
    "log_reg_gcn = LogisticRegression(penalty='l2', multi_class=\"auto\", solver=\"liblinear\", C=1e4, fit_intercept=False, max_iter=1000)\n",
    "log_reg_gcn.fit(train_features_gcn, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = log_reg_gcn.score(train_features_gcn, train_labels)\n",
    "val_acc = log_reg_gcn.score(val_features_gcn, val_labels)\n",
    "test_acc = log_reg_gcn.score(test_features_gcn, test_labels)\n",
    "\n",
    "print('Train accuracy {:.4f} | Validation accuracy {:.4f} | Test accuracy {:.4f}'.format(train_acc, val_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
